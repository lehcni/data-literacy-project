{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Literacy\n",
    "#### University of TÃ¼bingen, Winter Term 2021/22\n",
    "## Exercise Sheet 8\n",
    "&copy; 2021 Prof. Dr. Philipp Hennig & Agustinus Kristiadi\n",
    "\n",
    "This sheet is **due on Monday 20 December 2021 at 10am sharp (i.e. before the start of the lecture).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LFW Dataset\n",
    "\n",
    "**Task 1.** First order of business: Unzip the `data.zip` file. Then load the LFW [1] images (in the `images` directory) into a matrix $\\mathbf{X} \\in \\mathbb{R}^{13232 \\times 1764}$, where each row contains an image vector. Note that the image shape is $49 \\times 36$. \n",
    "\n",
    "#### References\n",
    "\n",
    "1. http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "\n",
    "N = 13232\n",
    "dim = (49, 36)\n",
    "\n",
    "X_lfw = np.zeros([N, 49*36])\n",
    "\n",
    "#################################################################\n",
    "#                                                               #\n",
    "#                        YOUR CODE HERE                         #\n",
    "#                                                               #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feeling about this dataset, here are 5 random images from LFW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(n_images):\n",
    "    idx = np.random.randint(len(X_lfw))\n",
    "    ax = fig.add_subplot(1, n_images, i+1)\n",
    "    ax.imshow(X_lfw[idx].reshape(dim), cmap='Greys_r')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "The first goal of this sheet is to understand PCA by implementing it. First we will revisit the step-by-step algorithm of PCA and then apply it to the LFW faces dataset. Then we will build our intuition about PCA as a dimensionality reduction technique which tries to minimize the reconstruction error of a given dataset. \n",
    "    \n",
    "As we have seen in the lecture, the algorithm for computing the projection matrix $\\mathbf{U}$ containing $k$ principal components is as follows:\n",
    "\n",
    "1. Compute the data mean $\\overline{\\mathbf{x}} := \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n$.\n",
    "2. Compute the centered data matrix $\\widehat{\\mathbf{X}} := (\\mathbf{x}_1 - \\overline{\\mathbf{x}}, \\dots, \\mathbf{x}_N - \\overline{\\mathbf{x}})^\\top$.\n",
    "2. Compute the covariance matrix $\\mathbf{C} = \\frac{1}{N-1} \\widehat{\\mathbf{X}}^\\top \\widehat{\\mathbf{X}}$.\n",
    "3. Do eigendecomposition on the covariance $\\mathbf{C} = \\mathbf{Q} \\, \\mathrm{diag}(\\boldsymbol{\\lambda}) \\, \\mathbf{Q}^\\top$.\n",
    "4. Pick $k$ columns of $\\mathbf{Q}$ corresponding to the $k$ largest eigenvalues in $\\boldsymbol{\\lambda}$, and arrange them as a matrix $\\mathbf{U}$.\n",
    "\n",
    "Given a data point $\\mathbf{x}$, we can compute the its reconstruction w.r.t. to the projection $\\mathbf{U}$ via the following steps:\n",
    "\n",
    "1. Project the point onto the latent space $\\mathbf{z} = \\mathbf{U}^\\top \\mathbf{x}$.\n",
    "2. Obtain the reconstruction by projecting the latent point back onto the data space $\\widetilde{\\mathbf{x}} = \\mathbf{U} \\mathbf{z}$.\n",
    "\n",
    "The reconstruction error $e$ is simply the mean-squared-error between the data $\\mathbf{X}$ and their reconstructions $\\widetilde{\\mathbf{X}}$, i.e.\n",
    "\n",
    "$$\n",
    "    e = \\frac{1}{N} \\sum_{n=1}^N \\Vert \\mathbf{x}_n - \\widetilde{\\mathbf{x}}_n \\Vert^2_2 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Compute the covariance matrix $\\mathbf{C} \\in \\mathbb{R}^{1764 \\times 1764}$. And do eigendecomposition on $\\mathbf{C}$ to get $\\mathbf{Q}$ and $\\boldsymbol{\\lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#                        YOUR CODE HERE                         #\n",
    "#                                                               #\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the columns of $\\mathbf{Q}$ are basis vectors for the data. I.e., each LFW image can be written as a linear combination of these vectors. It is therefore interesting to see what do these basis vectors looks like as images. Let's show the first $5$ eigenvectors in $\\mathbf{Q}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(n_images):\n",
    "    ax = fig.add_subplot(1, n_images, i+1)\n",
    "    ax.imshow(Q[:, i].reshape(dim), cmap='Greys_r')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.** Compute the projected and reconstructed data using the first $k$ basis vectors. Compare an image with their corresponding reconstruction. Do you observe a relationship between $k$ and the reconstruction quality?\n",
    "\n",
    "_Hint:_ Since we arrange our data in a matrix, the formula for $\\mathbf{z}$ and $\\widetilde{\\mathbf{x}}$ might be slightly different. The main point is to get matrices $\\mathbf{Z} \\in \\mathbb{R}^{13232 \\times k}$ and $\\widetilde{\\mathbf{X}} \\in \\mathbb{R}^{13232 \\times 1764}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct(X, U):\n",
    "    \"\"\"\n",
    "    Reconstruct images using the first-k eigenvectors.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    X: Data matrix of shape (13232, 1764)\n",
    "    U: Projection matrix consisting of the first-k eigenvectors. The shape is (1764, k)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_tilde: The reconstructed data matrix. The shape is (13232, 1764)\n",
    "    \"\"\"\n",
    "    #################################################################\n",
    "    #                                                               #\n",
    "    #                        YOUR CODE HERE                         #\n",
    "    #                                                               #\n",
    "    #################################################################\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# The reconstruction of X using the first k basis vectors\n",
    "for k in [10, 200]:\n",
    "    X_tilde_lfw = reconstruct(X_lfw, Q[:, :k])\n",
    "\n",
    "    # Show a random original image and its reconstruction\n",
    "    idx = np.random.randint(len(X_lfw))\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.imshow(X_lfw[idx].reshape(dim), cmap='Greys_r')\n",
    "    ax1.title.set_text('Original')\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.imshow(X_tilde_lfw[idx].reshape(dim), cmap='Greys_r')\n",
    "    ax2.title.set_text('Reconstruction')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.** Let us now quantify the reconstruction error. Create a function that takes the original data and a projection matrix. Compare the errors when $k=5$ and $k=200$. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(X, U):\n",
    "    \"\"\"\n",
    "    Get the reconstruction error using the first-k eigenvectors.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    X: Data matrix of shape (13232, 1764)\n",
    "    U: Projection matrix consisting of the first-k eigenvectors. The shape is (1764, k)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    error: The mean-squared-error between the original data and the reconstruction\n",
    "    \"\"\"\n",
    "    #################################################################\n",
    "    #                                                               #\n",
    "    #                        YOUR CODE HERE                         #\n",
    "    #                                                               #\n",
    "    #################################################################\n",
    "\n",
    "\n",
    "print(f'{get_error(X_lfw, Q[:, :10]):.1f}')\n",
    "print(f'{get_error(X_lfw, Q[:, :200]):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "\n",
    "We have seen how we can do simple dimensionality reduction using PCA: just pick several eigenvectors. With $k=2$ latent dimension, a high-dimensional dataset is amenable for visualization. However, PCA is linear model---it might not be able to capture nonlinear relationships between the data and their embeddings. \n",
    "\n",
    "Nonlinear embedding algorithms have thus been proposed. t-SNE [2] uses a heavy-tailed function to measure distance between any pair of data. This way, we can represent the data as a collection of vertices and weighted edges---a graph. The normalized distances can further be interpreted as probabilities. The goal is then to find a graph, embedded in the latent space, that is close to the original data's graph as measured by KL-divergence.\n",
    "\n",
    "In this exercise, we implement t-SNE. The goal main goal is to understand the behind-the-scene of t-SNE better so that we can better utilize it. This is important since t-SNE's hyperparameters are impactful to the end results.\n",
    "\n",
    "#### References\n",
    "\n",
    "2. Maaten, Laurens van der, and Geoffrey Hinton. \"Visualizing data using t-SNE.\" Journal of machine learning research 9.Nov (2008): 2579-2605."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing t-SNE\n",
    "\n",
    "**Task 5.** Read the t-SNE paper, especially Section 1-3.\n",
    "\n",
    "\n",
    "For convenience here is a summary of the t-SNE's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='tsne_algo.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, $Perp$ is a hyperparameter that represent the target perplexity of low-dimensional representations. Meanwhile, $T$, $\\eta$, and $\\alpha(t)$ are gradient descent's hyperparameters for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Computing symmetric pairwise affinities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to compute each symmetric pairwise affinity $p_{ij}$ given a pair of points $x_i$ and $x_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as scidist\n",
    "\n",
    "\n",
    "# Pairwise squared Euclidean distance\n",
    "def get_dists(Z):\n",
    "    \"\"\"\n",
    "    Compute all pairwise distances of each data points z_i in Z.\n",
    "    \n",
    "    Params:\n",
    "        Z: matrix with n rows. Each row is the z_i\n",
    "        \n",
    "    Return:\n",
    "        All pairwise distances of each data points z_i in Z\n",
    "    \"\"\"\n",
    "    dists = scidist.pdist(Z, metric='sqeuclidean')  \n",
    "    return scidist.squareform(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recall that for each $i$ we need to compute $p_{j|i}$ that attains the desired perplexity value `perp`. The function for computing $\\beta_i$'s that achieve $Perp$ perplexity is available in `tsne_util.py`---it is simply a binary search algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tsne_util\n",
    "\n",
    "\n",
    "\n",
    "def get_perplexity(p):\n",
    "    \"\"\"\n",
    "    Returns the perplexity of p. See https://en.wikipedia.org/wiki/Perplexity\n",
    "    \n",
    "    Params:\n",
    "        p: probability vector\n",
    "        \n",
    "    Return:\n",
    "        A single number---the perplexity of p\n",
    "    \"\"\"\n",
    "    entropy = -np.sum(p * np.log2(p + 1e-10))\n",
    "    return 2**entropy\n",
    "\n",
    "\n",
    "def get_beta(perp, dists_X):\n",
    "    \"\"\"\n",
    "    Let beta_i := 2 \\sigma_i^2. This function computes (beta_i) that achieve\n",
    "    the desired perplexity.\n",
    "    \n",
    "    Params:                 \n",
    "        perp: Desired perplexity value.\n",
    "        \n",
    "        dists_X: Pairwise squared Euclidean distances between points in X, stored in an (n x n)-matrix\n",
    "                \n",
    "    Returns:\n",
    "        betas: (n,) array of beta_i's \n",
    "    \"\"\"    \n",
    "    return tsne_util.binary_search(perp, dists_X, get_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional probability $p_{j | i}$ is defined by\n",
    "\n",
    "$$\n",
    "    p_{j | i} := p(x_j | x_i) := \\frac{\\exp(-\\Vert x_i - x_j \\Vert^2) / \\beta_i}{\\sum_{k \\neq i} \\exp(-\\Vert x_i - x_k \\Vert^2 / \\beta_i)} \\qquad \\text{where} \\enspace p_{i|i} = 0 ,\n",
    "$$\n",
    "\n",
    "where each $\\beta_i$ is obtained in such a way that it (approximately) achieves the desired perplexity value $Perp$. Here is a function for computing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_j_given_i(dists_X, perp):\n",
    "    \"\"\"\n",
    "    Compute the conditional probabilities p_{j|i}'s\n",
    "    \n",
    "    Params\n",
    "        dists_X: pairwise-distances matrix of X\n",
    "        perp: the desired perplexity level (single number)\n",
    "\n",
    "    Return:\n",
    "        (n, n) matrix containing p_{j|i}'s\n",
    "    \"\"\"\n",
    "    betas = get_beta(perp, dists_X)\n",
    "\n",
    "    p_j_given_i = np.exp(-dists_X / betas[None, :])\n",
    "    np.fill_diagonal(p_j_given_i, 0)\n",
    "    p_j_given_i /= p_j_given_i.sum(1, keepdims=True)\n",
    "    \n",
    "    return p_j_given_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint probability $p_{ij}$ is\n",
    "\n",
    "$$\n",
    "    p_{ij} := \\frac{p_{j|i} + p_{i|j}}{2n} \\qquad \\text{where} \\enspace p_{ii} = 0 ,\n",
    "$$\n",
    "\n",
    "for all $i, j \\in \\{ 1, \\dots, n \\}$. We assume that the matrix containing all $p_{ij}$ is denoted by $P \\in \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "\n",
    "**Task 6.** Now, write a function for computing all the joint probabilities $p_{ij}$, i.e. computing $P$. You should end up with an $n \\times n$ matrix with $p_{ii} = 0$ for all $i=1, \\dots, n$, and $\\sum_{i=1}^n \\sum_{j=1}^n p_{ij} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_P(dists_X, perp):\n",
    "    \"\"\"\n",
    "    Compute the joint probabilities p_ij's\n",
    "    \n",
    "    Params\n",
    "        dists_X: pairwise-distances matrix of X\n",
    "        perp: the desired perplexity level (single number)\n",
    "\n",
    "    Return:\n",
    "        (n, n) matrix P containing p_ij's\n",
    "    \"\"\"\n",
    "    #################################################################\n",
    "    #                                                               #\n",
    "    #                        YOUR CODE HERE                         #\n",
    "    #                                                               #\n",
    "    #################################################################\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Computing best low-dimensional representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to address the steps inside the loop in the algorithm above. \n",
    "\n",
    "The low-dimensional affinities $q_{ij}$ is defined by \n",
    "\n",
    "$$\n",
    "    q_{ij} := \\frac{(1 + \\Vert y_i - y_j \\Vert^2)^{-1}}{\\sum_{k \\neq l} (1 + \\Vert y_k - y_l \\Vert^2)^{-1}}  \\qquad \\text{where} \\enspace q_{ii} = 0 ,\n",
    "$$\n",
    "\n",
    "for all $i, j \\in \\{1, \\dots, n\\}$. We assume that the matrix containing all $q_{ij}$ is denoted by $Q \\in \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "<!-- **Task.** Construct a function that outputs $q_{ij}$'s (i.e. $Q$) given distances between $y_i$ and $y_j$. The input of this function is an $n \\times n$ matrix containing all pairwise distances between $y_i$'s and $y_j$'s. The return value of this function should be an $n \\times n$ matrix $Q$, where each row is a valid probability distribution, i.e. $q_{ij} \\geq 1$ and $\\sum_{i=1}^n \\sum_{j=1}^n q_{ij} = 1$. Note that $q_{ii} = 0$ for all $i = 1, \\dots, n$. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q(dists_Y):    \n",
    "    \"\"\"\n",
    "    Compute low-dimensional affinities q_ij\n",
    "    \n",
    "    Params\n",
    "        dists_Y: (n, n) matrix containing all pairwise distances of elements of Y\n",
    "\n",
    "    Return:\n",
    "        (n, n) matrix Q containing q_ij's\n",
    "    \"\"\"\n",
    "    Q = 1/(1 + dists_Y)\n",
    "    np.fill_diagonal(Q, 0)\n",
    "    Q /= Q.sum()\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function---the function we are going to optimize in order to find an optimal embedding $\\mathcal{Y}$---is the KL-divergence between $P$ and $Q$:\n",
    "\n",
    "$$\n",
    "    C := D_\\text{KL}(P \\Vert Q) := \\sum_{i=1}^n \\sum_{j=1}^n p_{ij} \\log \\frac{p_{ij}}{q_{ij}} .\n",
    "$$\n",
    "\n",
    "Since we need to minimize $C$ via gradient descent, we need its gradient:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial y_i} = 4 \\sum_{j=1}^n \\frac{(p_{ij} - q_{ij}) (y_i - y_j)}{1 + \\Vert y_i - y_j \\Vert^2} .\n",
    "$$\n",
    "\n",
    "\n",
    "**Task 7.** For the final task in this step, write a function that computes the $n$ gradient vectors $\\left\\{ \\frac{\\partial C}{\\partial y_i} \\right\\}_{i=1}^n$ given $Y$, $P$, $Q$, and all pairwise distances between $y_i$ and $y_j$. Note that each gradient is a vector of length `N_PROJ_DIM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(Y, P, Q, dists_Y):\n",
    "    \"\"\"\n",
    "    Compute the KL-divergence gradient\n",
    "    \n",
    "    Params\n",
    "        Y: (n, N_PROJ_DIM) matrix\n",
    "        P: (n, n) matrix\n",
    "        Q: (n, n) matrix\n",
    "        dists_Y: (n, n) matrix containing all pairwise distances of elements of Y\n",
    "\n",
    "    Return:\n",
    "        (n, N_PROJ_DIM) matrix---each row is the gradient dC/dy_i\n",
    "    \"\"\"\n",
    "    #################################################################\n",
    "    #                                                               #\n",
    "    #                        YOUR CODE HERE                         #\n",
    "    #                                                               #\n",
    "    #################################################################\n",
    "    \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='tsne_algo.png', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have all the necessary ingredients, we are finally ready run the full algorithm. The implementation of the pseudocode is given below.\n",
    "\n",
    "**Note.** To test the algorithm, look at the plots generated by the test code below. Your aim is to be \"better\" than PCA---slightly worse than Scikit-Learn's implementation is to be expected. \"Better\" here is subjective, but the rule of thumb is to obtain a 2D representations that minimize **intra**-cluster distances and maximize **inter**-cluster distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to experiment with different values\n",
    "perp = 30\n",
    "\n",
    "\n",
    "\"\"\" DO NOT MODIFY ANYTHING BELOW THIS!!! \"\"\"\n",
    "\"\"\" ------------------------------------ \"\"\"\n",
    "\n",
    "from tqdm import trange\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "N_PROJ_DIM = 2\n",
    "\n",
    "# Load dataset\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "\n",
    "\n",
    "def my_tsne(X, perp, T=1000, eta=200):\n",
    "    np.random.seed(423)\n",
    "    \n",
    "    dists_X = get_dists(X)\n",
    "    P = get_P(dists_X, perp)\n",
    "    Y = np.random.randn(len(X), N_PROJ_DIM) * 10**(-2)  # Each sampled from N(0, 10^-4)\n",
    "    v = 0  # veloctiy for Momentum-Gradient-Descent\n",
    "\n",
    "    progress_bar = trange(T)\n",
    "\n",
    "    for t in progress_bar:\n",
    "        dists_Y = get_dists(Y)\n",
    "        Q = get_Q(dists_Y)\n",
    "\n",
    "        kl_loss = np.sum(P * (np.log(P+1e-10) - np.log(Q+1e-10)))\n",
    "\n",
    "        # An alternative way (relative to the one in the paper) to do gradient descent + momentum\n",
    "        grads = get_grad(Y, P, Q, dists_Y)\n",
    "        alpha = 0.5 if t < 250 else 0.8\n",
    "        v = alpha*v + eta*grads\n",
    "        Y -= v\n",
    "        \n",
    "        if t % T//20 == 0:\n",
    "            progress_bar.set_description(f'[KL-loss: {kl_loss:.3f}]')\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Running and evaluating the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run our own t-SNE implementation. To validate the correctness of our implementation, we compare the result of our t-SNE against `scikit-learn`'s t-SNE embeddings.\n",
    "\n",
    "_Note._ There will of course be some difference since our implementation is quite \"naive\" compare to `scikit-learn`'s. But, if you implemented it correctly, you will see similar embedding plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "Z = my_tsne(X_iris, perp=perp)\n",
    "axs[0].scatter(Z[:, 0], Z[:, 1], c=y_iris, edgecolors='k');\n",
    "axs[0].set_title('My t-SNE');\n",
    "\n",
    "# Compare against scikit-learn's tSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "Z_sklearn = TSNE(perplexity=perp, random_state=3).fit_transform(X_iris)\n",
    "\n",
    "axs[1].scatter(Z_sklearn[:, 0], Z_sklearn[:, 1], c=y_iris, edgecolors='k');\n",
    "axs[1].set_title('Scikit-learn\\'s t-SNE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.** Try different perplexity values and write your observations below. What happens when $Perp$ is very small, say $2$? What happens when it is very large, e.g. $1000$? The original t-SNE paper recommends perplexity values between $5$ and $50$. Do you agree with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, here we see that t-SNE is _not_ fool-proof! On the (subset of the) LFW dataset, we can see that t-SNE does not give interpretable results with the default hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Label for each image (i.e. the sex of the person)\n",
    "y_lfw = np.loadtxt(f'data/images/sex.txt').ravel().astype('int8')\n",
    "\n",
    "# Subsample data to make things quicker\n",
    "X_lfw_small, y_lfw_small = resample(X_lfw, y_lfw, n_samples=200, random_state=99)\n",
    "\n",
    "# Do t-SNE\n",
    "Z_lfw = my_tsne(X_lfw_small, perp=30)\n",
    "\n",
    "plt.scatter(Z_lfw[:, 0], Z_lfw[:, 1], c=y_lfw_small, edgecolors='k');\n",
    "plt.title('t-SNE on LFW');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
