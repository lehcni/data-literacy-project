{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Literacy\n",
    "#### University of Tübingen, Winter Term 2021/22\n",
    "## Exercise Sheet 5\n",
    "&copy; 2021 Prof. Dr. Philipp Hennig & Lukas Tatzel\n",
    "\n",
    "This sheet is **due on Monday 29 November 2021 at 10 am sharp (i.e. before the start of the lecture).**\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis Testing & *Hunting* for Significance \n",
    "\n",
    "**What is this week's tutorial about?** In this week's tutorial, we will analyze data from the 1. Fußball-Bundesliga (the German premier soccer league). The goal is to investigate the effect of the Corona pandemic on the teams' performances. More precisely, we will try to find teams that achieved significantly worse results in the first \"Corona-year\" 2020 than in previous years (possibly caused by empty stadiums, etc.). For that purpose, we will conduct multiple hypotheses tests and discuss whether and how we should therefore adapt our strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:44.327461Z",
     "iopub.status.busy": "2020-12-03T09:18:44.327152Z",
     "iopub.status.idle": "2020-12-03T09:18:44.730626Z",
     "shell.execute_reply": "2020-12-03T09:18:44.729897Z",
     "shell.execute_reply.started": "2020-12-03T09:18:44.327397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make inline plots vector graphics\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and prepare the Data\n",
    "\n",
    "In this tutorial, we will use data provided by [OpenLigaDB](https://www.openligadb.de/). The function `get_league_table` allows you to retrieve the data via the API interface and convert it into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:45.089914Z",
     "iopub.status.busy": "2020-12-03T09:18:45.089660Z",
     "iopub.status.idle": "2020-12-03T09:18:45.300391Z",
     "shell.execute_reply": "2020-12-03T09:18:45.299727Z",
     "shell.execute_reply.started": "2020-12-03T09:18:45.089890Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_ENDPOINT = \"https://www.openligadb.de/api\"\n",
    "\n",
    "def get_league_table(league, year):\n",
    "    \"\"\"\n",
    "    Get team rankings as Pandas data frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    league : str\n",
    "        'bl1' for 1. Bundesliga, see https://github.com/OpenLigaDB/OpenLigaDB-Samples\n",
    "    year : int\n",
    "        Get data for this year\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{API_ENDPOINT}/getbltable/{league}/{year}\")\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Get and display data for the 1. Bundesliga ('bl1') for 2020\n",
    "data = get_league_table(league=\"bl1\", year=2020)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Your first task is to essentially recreate the following table:\n",
    "\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"Table.PNG\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "**Recommended Steps:**\n",
    "1. First, gather data for all years from 2010 to 2020 and store this data into a single Pandas data frame. Each time you receive a data frame from `get_league_table`, extract the relevant columns and add the corresponding year as a new column `\"year\"`.\n",
    "2. Split this data frame into two separate data frames: One for the year 2020, the other one for the accumulated results from 2010 to 2019. \n",
    "3. Merge the two data frames on the `\"TeamName\"`-column. Make sure that you only include teams that played in 2020 and also in at least one other season in the past. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:45.709439Z",
     "iopub.status.busy": "2020-12-03T09:18:45.709154Z",
     "iopub.status.idle": "2020-12-03T09:18:46.947078Z",
     "shell.execute_reply": "2020-12-03T09:18:46.946267Z",
     "shell.execute_reply.started": "2020-12-03T09:18:45.709410Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute p-Values\n",
    "\n",
    "Our goal is to find out if there are teams that achieved significantly worse results in 2020 than in previous years. In the lecture, you already learned about a statistical test for this purpose: \n",
    "\n",
    "- First, we put a beta-prior on $f_0$ (the winning probability before 2020) which is based on $m_0$ (the number of wins before 2020) in $n_0$ matches (the number of matches before 2020).\n",
    "\n",
    "- Under the null hypothesis $H_0: f_1 = f_0$, the number of wins in 2020 $m_1$ (given the number of matches in 2020 $n_1$) follows a binomial distribution. \n",
    "\n",
    "- Putting these building blocks together, we obtain a [beta-binomial distribution](https://en.wikipedia.org/wiki/Beta-binomial_distribution)\n",
    "\n",
    "    \\begin{equation}\n",
    "    p(m_1 \\vert n_1, m_0, n_0) \n",
    "    = {n_1\\choose m_1} \n",
    "    \\frac{\\mathcal{B}(m_0 + m_1 + 1, (n_0-m_0) + (n_1-m_1) + 1)}\n",
    "    {\\mathcal{B}(m_0 + 1, n_0 - m_0 + 1)}.\n",
    "    \\end{equation}\n",
    "\n",
    "    This tells us the probability to observe $m_1$ wins in 2020, given the number of matches in 2020 $n_1$ and the statistics $m_0$, $n_0$ for the years before. \n",
    "\n",
    "**Task:** Plot $p(m_1 \\vert n_1, m_0, n_0)$ for $m_1 \\in \\{0, 1, ..., n_1\\}$ for some arbitrary numbers $n_1$, $m_0$ and $n_0$. Play around with these parameters to gain some intuition how this distribution behaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import betabinom\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The $p$-value represents the probability to observe a certain number of wins or *more extreme* results. Let's assume, the team we consider has won $3$ times in 2020. Since we are interested in teams that have played particularly badly, we sum $p(m_1 \\vert n_1, m_0, n_0)$ over $m_1 = 0, 1, 2, 3$. Evaluating the cumulative distribution function `betabinom.cdf` performs this summation for us. \n",
    "\n",
    "- If this probability is small, then it is very unlikely that the observed data has been generated from the winning probability $f_0$. Or, in other words, the team has played particularly badly in 2020. We thus reject $H_0$ if $p \\leq \\alpha := 5 \\%$. \n",
    "\n",
    "**Task:** Compute the $p$-values for every team and add the results as a new column `\"p-value (won)\"`. Check if there are teams whose $p$-value falls below the $5 \\%$ threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:46.979525Z",
     "iopub.status.busy": "2020-12-03T09:18:46.979289Z",
     "iopub.status.idle": "2020-12-03T09:18:47.562397Z",
     "shell.execute_reply": "2020-12-03T09:18:47.561685Z",
     "shell.execute_reply.started": "2020-12-03T09:18:46.979502Z"
    }
   },
   "outputs": [],
   "source": [
    "def p_val_won(m_1, n_1, m_0, n_0):\n",
    "    \"\"\"\n",
    "    Compute p-value by summing the evidence p(m_1 | n_1, m_0, n_0) over the \n",
    "    observed number of wins and 'more extreme' (i.e. smaller) results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m_1 : int\n",
    "        Number of wins in 2020 (0 <= m_1 <= n_1)\n",
    "    n_1 : int\n",
    "        Number of matches in 2020 (n_1 > 0)\n",
    "    m_0 : int\n",
    "        Number of wins before 2020 (0 <= m_0 <= n_0)\n",
    "    n_0 : int\n",
    "        Number of matches before 2020 (n_0 > 0)\n",
    "    \"\"\"\n",
    "    return betabinom.cdf(m_1, n_1, m_0 + 1, n_0 - m_0 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:47.564144Z",
     "iopub.status.busy": "2020-12-03T09:18:47.563868Z",
     "iopub.status.idle": "2020-12-03T09:18:47.595089Z",
     "shell.execute_reply": "2020-12-03T09:18:47.594130Z",
     "shell.execute_reply.started": "2020-12-03T09:18:47.564116Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find teams that played significantly worse in 2020 compared to previous years. We did this by checking whether the number of games won this year is \"surprisingly\" low. Now, we will use an alternative approach: We check whether the number of games *lost* this year is particularly *high*. For this, we can re-use the statistical beta-binomial model from above by simply plugging in the number of *lost* games for $m_0$ and $m_1$. \n",
    "\n",
    "**Task:** Compute the corresponding $p$-values and store the results in a new column `\"p-value (lost)\"`. Note that you cannot simply use the `p_val_won` function from above. This time, the question is: What is the probability for observing $m_1$ or *more* lost matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:47.659651Z",
     "iopub.status.busy": "2020-12-03T09:18:47.659397Z",
     "iopub.status.idle": "2020-12-03T09:18:47.691939Z",
     "shell.execute_reply": "2020-12-03T09:18:47.691178Z",
     "shell.execute_reply.started": "2020-12-03T09:18:47.659628Z"
    }
   },
   "outputs": [],
   "source": [
    "def p_val_lost(m_1, n_1, m_0, n_0):\n",
    "    \"\"\"\n",
    "    Compute p-value by summing the evidence p(m_1 | n_1, m_0, n_0) over the \n",
    "    observed number of lost matches and 'more extreme' (i.e. larger) results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m_1 : int\n",
    "        Number of lost matches in 2020 (0 <= m_1 <= n_1)\n",
    "    n_1 : int\n",
    "        Number of matches in 2020 (n_1 > 0)\n",
    "    m_0 : int\n",
    "        Number of lost matches before 2020 (0 <= m_0 <= n_0)\n",
    "    n_0 : int\n",
    "        Number of matches before 2020 (n_0 > 0)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bonferroni Correction\n",
    "\n",
    "By now, we conducted $2 \\cdot 17 = 34$ hypotheses tests at the significance level $\\alpha = 5 \\%$. $\\alpha$ corresponds to the probability of a type I error, i.e. rejecting the null hypothesis given that it is true. However, the more tests we perform, the higher the chance of observing a rare event simply due to chance. For example, if we assume that $H_0$ holds for every team, the chance of falsely rejecting at least one out of $34$ hypothesis is $1 - (1-0.05)^{34} \\approx 83 \\%$ (assuming independent tests). Thus, it is quite likely that one of the results we found is a type I error. The Bonferroni correction is one possibility for compensating for that effect by decreasing the significance level. The significance level is defined as the original one divided by the total number of hypotheses. \n",
    "\n",
    "**Task:** Define the new significance level and see whether you can (still) find significant results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:48.365359Z",
     "iopub.status.busy": "2020-12-03T09:18:48.365108Z",
     "iopub.status.idle": "2020-12-03T09:18:48.371750Z",
     "shell.execute_reply": "2020-12-03T09:18:48.371072Z",
     "shell.execute_reply.started": "2020-12-03T09:18:48.365336Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bonferroni method tends to be too *conservative*, i.e. the significance level might be too restrictive. This is especially the case when the tests are dependent. \n",
    "\n",
    "**Task:** Think about if the tests we performed are dependent or independent and give a short explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Q-Q-Plots\n",
    "\n",
    "We conducted multiple hypothesis tests at significance level $\\alpha = 5 \\%$. $\\alpha$ corresponds to the probability of a type I error, i.e. $\\alpha = P(\\text{type 1 error}) = P(p \\leq \\alpha\\,|\\,H_0\\,\\text{ is true}) = \\text{cdf}_p(\\alpha)$. So, under $H_0$, the cumulative distribution function of the $p$-value at $\\alpha$ is $\\text{cdf}_p(\\alpha) = \\alpha$. That implies that $p$ is uniformly distributed under $H_0$. Let us visually explore, if the observed $p$-values are likely to be drawn from a uniform distribution. This can be done by a so-called Q-Q plot. \n",
    "\n",
    "The idea of a Q-Q Plot is to compare the empirical quantiles of the empirical distribution of the $p$-values with the quantiles of the theoretical distribution (the uniform distribution, as explained above). Let $\\beta \\in (0, 1)$. \n",
    "- The theoretical $\\beta$-quantile of the uniform distribution is $q_{\\beta} = \\beta$.\n",
    "- The empirical $\\beta$-quantile of the *ascendingly ordered* sample $(p_1, ..., p_n)$ is $q_{\\beta} = p_{\\lfloor n\\cdot\\beta + 1\\rfloor}$\n",
    "\n",
    "**Task:** Complement the two functions below such that they return the quantiles defined above. Do not use `numpy.quantile` or similar.  Generate a vector that discretizes the variable $\\beta$ and compute the corresponding quantiles. Plot the theoretical quantiles over the empirical quantiles. If the distributions are *similar*, the points will lie on the $45°$ line $y = x$. The result should look like this: \n",
    "\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"Plot.PNG\" width=\"350\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:49.818145Z",
     "iopub.status.busy": "2020-12-03T09:18:49.817894Z",
     "iopub.status.idle": "2020-12-03T09:18:49.823192Z",
     "shell.execute_reply": "2020-12-03T09:18:49.822093Z",
     "shell.execute_reply.started": "2020-12-03T09:18:49.818121Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def q_theoretical(beta):\n",
    "    \"\"\"\n",
    "    Compute theoretical beta-quantile of uniform distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : array-like, shape=(n,)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "def q_empirical(beta, p):\n",
    "    \"\"\"\n",
    "    Compute empirical beta-quantile of sample p.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : array-like, shape=(n,)\n",
    "    p : array-like, shape=(n,)\n",
    "        Unordered sample\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:18:50.506118Z",
     "iopub.status.busy": "2020-12-03T09:18:50.505864Z",
     "iopub.status.idle": "2020-12-03T09:19:01.843109Z",
     "shell.execute_reply": "2020-12-03T09:19:01.842432Z",
     "shell.execute_reply.started": "2020-12-03T09:18:50.506095Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the two generated lines do not coincide perfectly with the $45°$ line $y = x$. That raises the question, what deviation from that line we would expect if the $p$-values were actually drawn from a uniform distribution. One way to approach this question visually is to generate multiple samples of a uniform distribution (each sample consisting of $17$ numbers, like `p-values (won)` and `p-values (lost)`) and make a Q-Q Plot for each sample. So, we basically sample Q-Q plots under $H_0$. \n",
    "\n",
    "**Task:** Generate multiple (e.g. $1000$) samples as described above and show the corresponding Q-Q Plots together with the two lines from the previous task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-03T09:25:47.023729Z",
     "iopub.status.busy": "2020-12-03T09:25:47.023411Z",
     "iopub.status.idle": "2020-12-03T09:25:47.468593Z",
     "shell.execute_reply": "2020-12-03T09:25:47.467972Z",
     "shell.execute_reply.started": "2020-12-03T09:25:47.023696Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
